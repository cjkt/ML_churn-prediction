# -*- coding: utf-8 -*-
"""2_feature_engineering_5_transformation_only.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ULiG2wKgEBGdqRx-DJv8vUoKfWjFrin0
"""

# install dependencies
!pip install dask
!pip install featuretools
!pip install -U -q PyDrive

from IPython.display import clear_output
clear_output()

# clean workspace
!rm -rf top_features
!rm -rf data
!rm -rf partitioned_data
!rm data.zip
!rm -rf __MACOSX
!ls

import featuretools as ft
from featuretools.primitives import *
import woodwork as ww
import pandas as pd
import numpy as np
import os
from random import sample
from tqdm import tqdm
from collections import Counter
ft.__version__

from google.colab import files
uploaded = files.upload()
import io
df = pd.read_csv(io.BytesIO(uploaded['churn-data_8_preDFS_not_encoded.csv']))

data = df.drop(df.columns[0], axis=1)
data

data = df.drop(df.columns[0], axis=1)
data =data.dropna()
data.shape

# outlier
def detect_outliers(df,n,features):
    """
    Takes a dataframe df of features and returns a list of the indices
    corresponding to the observations containing more than n outliers according
    to the Tukey method.
    """
    outlier_indices = []
    
    # iterate over features(columns)
    for col in features:
        # 1st quartile (25%)
        Q1 = np.percentile(df[col], 25)
        # 3rd quartile (75%)
        Q3 = np.percentile(df[col],75)
        # Interquartile range (IQR)
        IQR = Q3 - Q1
        
        # outlier step
        outlier_step = 1.5 * IQR
        
        # Determine a list of indices of outliers for feature col
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index
        
        # append the found outlier indices for col to the list of outlier indices 
        outlier_indices.extend(outlier_list_col)
        
    # select observations containing more than 2 outliers
    outlier_indices = Counter(outlier_indices)        
    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )
    
    return multiple_outliers   

# detect outliers from Age, SibSp , Parch and Fare
Outliers_to_drop = detect_outliers(data,2,["user_months","phones_used","handset_price","handset_age"])
data.loc[Outliers_to_drop]

# scailing
# after engineering

# random sampling (10k)
#data = data.sample(frac =.1)
#data.shape

#X = data.iloc[:,:13]
#Y = data.loc[:,'churn']
#print(X.shape, Y.shape)

data.info()

##### trans primitives

data2 = df.drop(df.columns[0], axis=1)
data2 =data2.dropna()
data2.shape

# detect outliers from Age, SibSp , Parch and Fare
Outliers_to_drop = detect_outliers(data2,2,["user_months","phones_used","handset_price","handset_age"])
data2.loc[Outliers_to_drop]

# Drop outliers
data2 = data2.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)
data2.shape

data.info()

data.ww.init(name='churns',index='Customer_ID')

es2 = ft.EntitySet(id="churn_data2")
es2 = es2.add_dataframe(dataframe_name="churns2",
           dataframe=data2,
           index="Customer_ID"
           #time_index="transaction_time",
           )

es2

es2['churns2'].ww.set_types(logical_types={'area': 'Categorical',
                                         'user_months':'Age',
                                         'new_user':'Categorical',
                                         'phones_used':'Integer',
                                         'handset_price':'Double',
                                         'handset_age':'Age',
                                         'refurb_or_new':'Boolean',
                                         'dualband':'Categorical',
                                         'web_capable':'Categorical',
                                         'manual_limit':'Boolean',
                                         'PRIZM_code':'Categorical',                                      
                                         'credit_card':'Boolean',
                                         'cred_score':'Integer'
                                         #'cred_score':'Ordinal'
                                         #'churn':'Boolean'
                                         })
es2['churns2'].ww

es2

ft.list_primitives()

features2, feature_names2 = ft.dfs(entityset=es2,
                                      target_dataframe_name="churns2",
                                      ignore_columns={'churns2':['churn']},
                                 trans_primitives=['less_than_equal_to', 'modulo_numeric', 'month', 'rolling_mean',
       'url_to_domain', 'cum_max', 'cityblock_distance',
       'subtract_numeric_scalar', 'tangent', 'less_than_equal_to_scalar',
       'modulo_numeric_scalar', 'rolling_std', 'second',
       'url_to_protocol', 'age', 'geomidpoint', 'num_characters',
       'scalar_subtract_numeric_feature', 'time_since', 'url_to_tld',
       'rolling_count', 'modulo_by_feature', 'equal', 'date_to_holiday',
       'greater_than', 'haversine', 'multiply_numeric', 'num_words',
       'and', 'equal_scalar', 'is_free_email_domain', 'is_null',
       'time_since_previous', 'day', 'greater_than_scalar', 'isin',
       'is_in_geobox', 'multiply_numeric_scalar', 'absolute',
       'email_address_to_domain', 'not_equal', 'or', 'diff',
       'distance_to_holiday', 'greater_than_equal_to', 'latitude',
       'multiply_boolean', 'cum_sum', 'not_equal_scalar', 'numeric_lag',
       'square_root', 'week', 'divide_numeric',
       'greater_than_equal_to_scalar', 'hour', 'longitude', 'negate',
       'add_numeric', 'cum_count', 'natural_logarithm', 'weekday',
       'divide_numeric_scalar', 'is_weekend', 'less_than', 'not',
       'rolling_max', 'add_numeric_scalar', 'cum_mean', 'sine', 'year',
       'divide_by_feature', 'less_than_scalar', 'minute', 'percentile',
       'rolling_min', 'cosine', 'cum_min', 'is_federal_holiday',
       'subtract_numeric'])
len(feature_names2)

feature_names2

features2

# combine two tables

features2.head()

# save not encoded
# concatonating target
#target = data[['Customer_ID','churn']]
#not_encoded_data = pd.merge(merged,target,on='Customer_ID')

#from google.colab import files
#not_encoded_data.to_csv('churn-data_10_DFS_NOT_encoded.csv') 
#files.download('churn-data_10_DFS_NOT_encoded.csv')

# encoding
# Determination categorical features
numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']
categorical_columns = []
cols = features2.columns.values.tolist()
for col in cols:
    if features2[col].dtype in numerics: continue
    categorical_columns.append(col)
categorical_columns

from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import explained_variance_score
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2

encoded = pd.get_dummies(features2, columns=categorical_columns)

encoded.head(3)

# Encoding categorical features (Label encoder)
#for col in categorical_columns:
#    if col in features.columns:
#        le = LabelEncoder()
#        le.fit(list(features[col].astype(str).values))
#        features[col] = le.transform(list(features[col].astype(str).values))

# Encoding categorical features (Label encoder)
#for col in categorical_columns:
#    if col in features.columns:

#        le = OneHotEncoder(sparse=False)
#        le.fit(list(features[col].astype(str).values))
#        features[col] = le.transform(list(features[col].astype(str).values))

encoded.head(3)

# feature selection with pearson's correlation

# Threshold for removing correlated variables
threshold = 0.9

def highlight(value):
    if value > threshold:
        style = 'background-color: pink'
    else:
        style = 'background-color: palegreen'
    return style

# Absolute value correlation matrix
corr_matrix = encoded.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
upper.style.applymap(highlight)

# Select columns with correlations above threshold
collinear_features = [column for column in upper.columns if any(upper[column] > threshold)]
features_filtered = encoded.drop(columns = collinear_features)
#features_positive = features_filtered.loc[:, features_filtered.ge(0).all()]
print('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])

# PCA

"""Data normalization"""

# data scailing (minmax)
def maximum_absolute_scaling(df):
    # copy the dataframe
    df_scaled = df.copy()
    # apply maximum absolute scaling
    for column in df_scaled.columns:
        df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()
    return df_scaled

scaled_1 = maximum_absolute_scaling(encoded) # before pearson's
scaled_2 = maximum_absolute_scaling(features_filtered) # pearson's filtered
print(scaled_1.shape, scaled_2.shape)

# pre selection
FE_option0 = scaled_1.columns 
# pearson's selection
FE_option1 = scaled_2.columns
print(len(FE_option0), len(FE_option1))

# concatonating target
target = data[['Customer_ID','churn']]
final_data_1 = pd.merge(scaled_1,target,on='Customer_ID')
final_data_2 = pd.merge(scaled_2,target,on='Customer_ID')
print(final_data_1.shape, final_data_2.shape)

#from google.colab import files
final_data_1.to_csv('churn-data_11_DFS_trans_only_no-selection.csv') 
files.download('churn-data_11_DFS_trans_only_no-selection.csv')

final_data_2.to_csv('churn-data_11_DFS_trans_only_pearsons-selected.csv') 
files.download('churn-data_11_DFS_trans_only_pearsons-selected.csv')

final_data_1.head(3)

final_data_2.head(3)