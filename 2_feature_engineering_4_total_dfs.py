# -*- coding: utf-8 -*-
"""2_feature_engineering_4_total_DFS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    
"""

# install dependencies
!pip install dask
!pip install featuretools
!pip install -U -q PyDrive

from IPython.display import clear_output
clear_output()

# clean workspace
!rm -rf top_features
!rm -rf data
!rm -rf partitioned_data
!rm data.zip
!rm -rf __MACOSX
!ls

import featuretools as ft
from featuretools.primitives import *
import woodwork as ww
import pandas as pd
import numpy as np
import os
from random import sample
from tqdm import tqdm
from collections import Counter
ft.__version__

from google.colab import files
uploaded = files.upload()
import io
df = pd.read_csv(io.BytesIO(uploaded['churn-data_8_preDFS_not_encoded.csv']))

data = df.drop(df.columns[0], axis=1)
data

data = df.drop(df.columns[0], axis=1)
data =data.dropna()
data.shape

# outlier
def detect_outliers(df,n,features):
    """
    Takes a dataframe df of features and returns a list of the indices
    corresponding to the observations containing more than n outliers according
    to the Tukey method.
    """
    outlier_indices = []
    
    # iterate over features(columns)
    for col in features:
        # 1st quartile (25%)
        Q1 = np.percentile(df[col], 25)
        # 3rd quartile (75%)
        Q3 = np.percentile(df[col],75)
        # Interquartile range (IQR)
        IQR = Q3 - Q1
        
        # outlier step
        outlier_step = 1.5 * IQR
        
        # Determine a list of indices of outliers for feature col
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index
        
        # append the found outlier indices for col to the list of outlier indices 
        outlier_indices.extend(outlier_list_col)
        
    # select observations containing more than 2 outliers
    outlier_indices = Counter(outlier_indices)        
    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )
    
    return multiple_outliers   

# detect outliers from Age, SibSp , Parch and Fare
Outliers_to_drop = detect_outliers(data,2,["user_months","phones_used","handset_price","handset_age"])
data.loc[Outliers_to_drop]

# Drop outliers
data = data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)

data.shape

# scailing
# after engineering

# random sampling (10k)
#data = data.sample(frac =.1)
#data.shape

#X = data.iloc[:,:13]
#Y = data.loc[:,'churn']
#print(X.shape, Y.shape)

data.info()

data.ww.init(name='churns',index='Customer_ID')

es = ft.EntitySet(id="churn_data")
es = es.add_dataframe(dataframe_name="churns",
           dataframe=data,
           index="Customer_ID"
           #time_index="transaction_time",
           )

es['churns'].ww.set_types(logical_types={'area': 'Categorical',
                                         'user_months':'Age',
                                         'new_user':'Categorical',
                                         'phones_used':'Integer',
                                         'handset_price':'Double',
                                         'handset_age':'Age',
                                         'refurb_or_new':'Boolean',
                                         'dualband':'Categorical',
                                         'web_capable':'Categorical',
                                         'manual_limit':'Boolean',
                                         'PRIZM_code':'Categorical',                                      
                                         'credit_card':'Boolean',
                                         'cred_score':'Integer'
                                         #'cred_score':'Ordinal'
                                         #'churn':'Boolean'
                                         })
es['churns'].ww

es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='area', index='area')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='user_months', index='user_months')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='new_user', index='new_user')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='phones_used', index='phones_used')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='handset_price', index='handset_price')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='handset_age', index='handset_age')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='refurb_or_new', index='refurb_or_new')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='dualband', index='dualband')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='web_capable', index='web_capable')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='manual_limit', index='manual_limit')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='PRIZM_code', index='PRIZM_code')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='credit_card', index='credit_card')
es = es.normalize_dataframe(base_dataframe_name='churns', new_dataframe_name='cred_score', index='cred_score')
es

primitives = ft.list_primitives()
pd.options.display.max_colwidth = 100
primitives[primitives['type'] == 'aggregation'].head(primitives[primitives['type'] == 'aggregation'].shape[0])

primitives = ft.list_primitives()
a = primitives["name"].to_numpy()
a

features, feature_names = ft.dfs(entityset=es,
                                      target_dataframe_name="churns",
                                      ignore_columns={'churns':['churn']},
                                 agg_primitives=['entropy','mode','count','num_unique'],
                                      max_depth=2)
len(feature_names)

feature_names

features

##### trans primitives

data2 = df.drop(df.columns[0], axis=1)
data2 =data2.dropna()
data2.shape

# detect outliers from Age, SibSp , Parch and Fare
Outliers_to_drop = detect_outliers(data2,2,["user_months","phones_used","handset_price","handset_age"])
data2.loc[Outliers_to_drop]

# Drop outliers
data2 = data2.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)
data2.shape

data.info()

data.ww.init(name='churns',index='Customer_ID')

es2 = ft.EntitySet(id="churn_data2")
es2 = es2.add_dataframe(dataframe_name="churns2",
           dataframe=data2,
           index="Customer_ID"
           #time_index="transaction_time",
           )

es2

es2['churns2'].ww.set_types(logical_types={'area': 'Categorical',
                                         'user_months':'Age',
                                         'new_user':'Categorical',
                                         'phones_used':'Integer',
                                         'handset_price':'Double',
                                         'handset_age':'Age',
                                         'refurb_or_new':'Boolean',
                                         'dualband':'Categorical',
                                         'web_capable':'Categorical',
                                         'manual_limit':'Boolean',
                                         'PRIZM_code':'Categorical',                                      
                                         'credit_card':'Boolean',
                                         'cred_score':'Integer'
                                         #'cred_score':'Ordinal'
                                         #'churn':'Boolean'
                                         })
es2['churns2'].ww

es2

features2, feature_names2 = ft.dfs(entityset=es2,
                                      target_dataframe_name="churns2",
                                      ignore_columns={'churns2':['churn']},
                                 trans_primitives=['square_root','cum_sum','divide_numeric','add_numeric','cum_count','natural_logarithm',
                                                   'cum_mean','percentile','modulo_numeric','cosine','cum_max','multiply_numeric'])
len(feature_names2)

feature_names2

features2

# combine two tables

features.head()

features2.head()

features3 = features.drop(features.columns[0:13],axis=1)

merged = pd.merge(features2,features3,on='Customer_ID')

merged.head()

# save not encoded
# concatonating target
#target = data[['Customer_ID','churn']]
#not_encoded_data = pd.merge(merged,target,on='Customer_ID')

#from google.colab import files
#not_encoded_data.to_csv('churn-data_10_DFS_NOT_encoded.csv') 
#files.download('churn-data_10_DFS_NOT_encoded.csv')

# encoding
# Determination categorical features
numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']
categorical_columns = []
cols = merged.columns.values.tolist()
for col in cols:
    if merged[col].dtype in numerics: continue
    categorical_columns.append(col)
categorical_columns

from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import explained_variance_score
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2

encoded = pd.get_dummies(merged, columns=categorical_columns)

encoded.head(3)

# Encoding categorical features (Label encoder)
#for col in categorical_columns:
#    if col in features.columns:
#        le = LabelEncoder()
#        le.fit(list(features[col].astype(str).values))
#        features[col] = le.transform(list(features[col].astype(str).values))

# Encoding categorical features (Label encoder)
#for col in categorical_columns:
#    if col in features.columns:

#        le = OneHotEncoder(sparse=False)
#        le.fit(list(features[col].astype(str).values))
#        features[col] = le.transform(list(features[col].astype(str).values))

features.head(3)

encoded.head(3)

# feature selection with pearson's correlation

# Threshold for removing correlated variables
threshold = 0.9

def highlight(value):
    if value > threshold:
        style = 'background-color: pink'
    else:
        style = 'background-color: palegreen'
    return style

# Absolute value correlation matrix
corr_matrix = encoded.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
upper.style.applymap(highlight)

# Select columns with correlations above threshold
collinear_features = [column for column in upper.columns if any(upper[column] > threshold)]
features_filtered = encoded.drop(columns = collinear_features)
#features_positive = features_filtered.loc[:, features_filtered.ge(0).all()]
print('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])

# PCA

"""Data normalization"""

# data scailing (minmax)
def maximum_absolute_scaling(df):
    # copy the dataframe
    df_scaled = df.copy()
    # apply maximum absolute scaling
    for column in df_scaled.columns:
        df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()
    return df_scaled

scaled_1 = maximum_absolute_scaling(encoded) # before pearson's
scaled_2 = maximum_absolute_scaling(features_filtered) # pearson's filtered
print(scaled_1.shape, scaled_2.shape)

# pre selection
FE_option0 = scaled_1.columns 
# pearson's selection
FE_option1 = scaled_2.columns
print(len(FE_option0), len(FE_option1))

# concatonating target
target = data[['Customer_ID','churn']]
final_data_1 = pd.merge(scaled_1,target,on='Customer_ID')
final_data_2 = pd.merge(scaled_2,target,on='Customer_ID')
print(final_data_1.shape, final_data_2.shape)

#from google.colab import files
final_data_1.to_csv('churn-data_10_DFS_no-selection.csv') 
files.download('churn-data_10_DFS_no-selection.csv')

final_data_2.to_csv('churn-data_10_DFS_pearsons-selected.csv') 
files.download('churn-data_10_DFS_pearsons-selected.csv')

final_data_1.head(3)

final_data_2.head(3)
